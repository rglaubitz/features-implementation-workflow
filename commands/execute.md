# Execute Phase Implementation

**Command:** `/execute [phase-number]`
**Purpose:** Systematic implementation using task-manager structure
**Workflow:** Lean 5-step pattern (Load ‚Üí Execute ‚Üí Validate ‚Üí Track ‚Üí Next)

---

## Prerequisites

Before running `/execute`:

- ‚úÖ `/breakdown` completed (task-manager/ structure exists)
- ‚úÖ All previous phase dependencies met
- ‚úÖ Development environment ready (services running)

**If task-manager/ not found:** Error - Run `/breakdown IMPLEMENTATION.md [project-name]` first

---

## Workflow Overview

The lean 5-step execution pattern:

```
Step 1: Load Task Context (30 sec)
   ‚îî‚îÄ Auto-read phase tasks, show checklist

Step 2: Execute Subtasks Interactively (varies)
   ‚îî‚îÄ Work through subtasks with quality standards

Step 3: Validate Implementation (5-10 min)
   ‚îî‚îÄ Run tests, verify success criteria (BLOCKING quality gate)

Step 4: Track Progress (10 sec)
   ‚îî‚îÄ Auto-update task-manager/ READMEs

Step 5: Next Action (instant)
   ‚îî‚îÄ Show next task, recommend compacts
```

**Total overhead per phase:** ~10 minutes (vs 30+ minutes in legacy workflow)

---

## Step 1: Load Task Context üìÇ

**Auto-detect task-manager/ structure and load phase tasks.**

### Detection

Check for `task-manager/` directory in current project:

```bash
if [ -d "task-manager/" ]; then
    echo "‚úÖ Task Manager structure detected"
    TASK_MANAGER_MODE=true
else
    echo "‚ùå No task-manager/ found"
    echo "   Run: /breakdown IMPLEMENTATION.md [project-name]"
    exit 1
fi
```

### Load Phase Tasks

**Read these files automatically:**

1. **Master README** - `task-manager/README.md`
   - Overall project progress
   - All phases with completion status

2. **Phase Overview** - `task-manager/phase-{N}-*/README.md`
   - Current phase summary
   - All tasks for this phase
   - Phase dependencies

3. **Task Files** - `task-manager/phase-{N}-*/task-{N}.{M}-*.md`
   - Detailed task breakdown
   - Subtasks (2-4 hour actionable chunks)
   - Dependencies (what must be complete first)
   - Success criteria (measurable outcomes)
   - Research references (ADRs, docs with line numbers)
   - Test specifications (exact tests to create)

### Display Context

**Announce loaded context:**

```
üìÇ TASK CONTEXT LOADED

Phase {N}: {Phase Name}
Duration: {X} weeks
Status: {Current status}

Tasks for this phase:
‚îú‚îÄ Task {N}.1: {Name} ({Duration}, {Y} tests)
‚îÇ  ‚îú‚îÄ Subtask {N}.1.1: {Name} (4 hours)
‚îÇ  ‚îú‚îÄ Subtask {N}.1.2: {Name} (6 hours)
‚îÇ  ‚îú‚îÄ Subtask {N}.1.3: {Name} (6 hours)
‚îÇ  ‚îî‚îÄ Subtask {N}.1.4: {Name} (4 hours)
‚îÇ
‚îú‚îÄ Task {N}.2: {Name} ({Duration}, {Z} tests)
‚îÇ  ‚îú‚îÄ Subtask {N}.2.1: {Name} (4 hours)
‚îÇ  ‚îî‚îÄ ...
‚îÇ
‚îî‚îÄ ... (all tasks for phase)

Phase Totals:
  - Tasks: {X}
  - Subtasks: {Y}
  - Tests: {Z}
  - Estimated Duration: {N} weeks

Research References:
  - ADR-00{X}: {Title} (referenced in Task {N}.{M})
  - research/documentation/{topic}.md (lines {X}-{Y})

Dependencies Verified:
  ‚úì {Dependency 1} (Phase {N-1} complete)
  ‚úì {Dependency 2} ({Service} running)

‚úì All task files loaded
‚úì Research references extracted
‚úì Test specifications identified
‚úì Ready to implement
```

### Quality Gate

**Verify before proceeding:**

- ‚úÖ task-manager/ structure exists and is valid
- ‚úÖ All phase task files are readable
- ‚úÖ Dependencies from previous phases are met
- ‚úÖ Success criteria are clearly defined
- ‚úÖ Research references are accessible

**NO REDUNDANT PLANNING** - Just load and display what `/breakdown` already documented.

**Time:** ~30 seconds (automated file reading)

---

## Step 2: Execute Subtasks Interactively üíª

**Work through subtasks systematically with quality standards.**

### Task Execution Loop

**For each task in the phase:**

#### Display Current Task

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Task {N}.{M}: {Task Name}
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Duration: {X} hours
Tests: {Y} specifications
Status: ‚¨ú Not Started

Dependencies:
  {List dependencies or "None - ready to start"}

Success Criteria:
  1. {Criterion 1} (measurable outcome)
  2. {Criterion 2} (measurable outcome)
  3. {Criterion 3} (measurable outcome)
  ...

Research References:
  - {ADR or doc} (lines {X}-{Y}) - {Why relevant}
  - {ADR or doc} (lines {A}-{B}) - {Why relevant}

Subtasks:
  ‚¨ú {N}.{M}.1: {Name} (4 hours)
  ‚¨ú {N}.{M}.2: {Name} (6 hours)
  ‚¨ú {N}.{M}.3: {Name} (6 hours)
  ‚¨ú {N}.{M}.4: {Name} (4 hours)
```

### Subtask Execution

**For each subtask in the task:**

#### 1. Display Subtask Details

```
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Subtask {N}.{M}.{S}: {Name}
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

Duration: {X} hours
Status: ‚¨ú Not Started

Implementation Steps:
  {Copy steps directly from task file}

Files to Create/Modify:
  {List from task file}

Code Examples:
  {Reference to IMPLEMENTATION.md sections via task file}

Validation Commands:
  {Commands from task file to verify this subtask}
```

#### 2. Implement the Code

**Quality Standards (Applied Automatically):**

**Code Quality:**
- ‚úÖ Type hints everywhere (`def func(x: int) -> str:`)
- ‚úÖ Google-style docstrings for all classes/functions
- ‚úÖ Async/await where appropriate
- ‚úÖ Structured logging (`logger.info()`, not `print()`)
- ‚úÖ Proper error handling (specific exceptions, try/except)
- ‚úÖ No TODO/FIXME in production code

**Code Pattern:**
```python
"""Module-level docstring explaining purpose.

Author: Apex Infrastructure Team
Created: {YYYY-MM-DD}
"""

import logging
from typing import List, Dict, Any, Optional

logger = logging.getLogger(__name__)


class ComponentName:
    """Component description.

    This component handles {what it does}.

    Args:
        param1: Description.
        param2: Description.

    Examples:
        >>> component = ComponentName()
        >>> result = await component.method()
    """

    def __init__(self, param1: str, param2: Optional[int] = None):
        """Initialize component."""
        self.param1 = param1
        self.param2 = param2
        logger.info(f"Initialized {self.__class__.__name__}")

    async def method(self) -> Dict[str, Any]:
        """Method description.

        Returns:
            Dictionary with results.

        Raises:
            ValueError: If validation fails.
        """
        try:
            # Implementation
            result = await self._internal_method()
            logger.info(f"Method completed successfully")
            return result
        except Exception as e:
            logger.error(f"Method failed: {e}")
            raise
```

**File Organization:**
- ‚úÖ Modular design (200-500 lines per file)
- ‚úÖ Clear imports and dependencies
- ‚úÖ Logical file structure (group related functionality)

#### 3. Announce Progress

```
üìù Implementing Subtask {N}.{M}.{S}...

Creating/Modifying:
  - {File 1} ({NEW/MODIFY}) - {Purpose}
  - {File 2} ({NEW/MODIFY}) - {Purpose}

Lines: ~{Estimated lines}
```

#### 4. Validate Subtask

**Run validation commands from task file:**

```bash
# Example validation commands
alembic upgrade head                    # Database migration
python -c "from module import Class"    # Import verification
curl http://localhost:8000/endpoint     # API endpoint check
```

**Show results:**

```
Validation Results:
  ‚úì {Validation 1} passed
  ‚úì {Validation 2} passed
  ‚úì {Validation 3} passed
```

#### 5. Mark Subtask Complete

```
‚úÖ Subtask {N}.{M}.{S} Complete

Delivered:
  ‚úì {File 1} created ({X} lines)
  ‚úì {File 2} updated (+{Y} lines)
  ‚úì All validations passed

Time: {Actual time spent}
```

### Task Progress Tracking

**Show continuous progress:**

```
Task {N}.{M} Progress:
  ‚îú‚îÄ Subtasks: {S}/{TOTAL} complete ({XX}%)
  ‚îú‚îÄ Estimated remaining: {X} hours
  ‚îî‚îÄ Status: {On Track / Ahead / Behind}
```

**When all subtasks complete, proceed to Step 3 (Validate Implementation).**

**NO REDUNDANT DISCUSSION** - Just execute what's documented in subtask steps.

**Time:** Varies by subtask complexity (actual implementation work)

---

## Step 3: Validate Implementation ‚úÖ

**Run tests and verify success criteria - THIS IS A BLOCKING QUALITY GATE.**

### Automatic Test Generation

**Generate tests proactively (WITHOUT being asked):**

After implementing each task, IMMEDIATELY create comprehensive tests.

#### Test Structure

```python
# File: tests/unit/test_{component}.py (NEW)

"""Unit tests for {component}.

Tests verify:
- Core functionality
- Edge cases
- Error handling
- Integration points

Test Specifications: TESTING.md lines {X}-{Y}

Author: Apex Infrastructure Team
Created: {YYYY-MM-DD}
"""

import pytest
from unittest.mock import Mock, AsyncMock, patch

from apex_memory.{module} import ComponentName


@pytest.fixture
def component():
    """Fixture providing test component instance."""
    return ComponentName(param1="test-value")


@pytest.mark.asyncio
async def test_core_functionality(component):
    """Test primary use case.

    Validates: {Success criterion from task file}
    """
    result = await component.method()

    assert result is not None
    assert result["status"] == "success"


@pytest.mark.asyncio
async def test_edge_case_empty_input(component):
    """Test handling of empty input.

    Validates: Edge case from TESTING.md line {X}
    """
    result = await component.method_with_input("")

    assert result["status"] == "skipped"


@pytest.mark.asyncio
async def test_error_handling(component):
    """Test error handling for invalid input.

    Validates: Error handling requirement
    """
    with pytest.raises(ValueError):
        await component.method_with_validation("invalid")


@pytest.mark.asyncio
async def test_integration_with_external_service(component):
    """Test integration with external service.

    Validates: Integration point from task dependencies
    """
    with patch("apex_memory.external.service_call") as mock_service:
        mock_service.return_value = {"data": "test"}

        result = await component.call_external_service()

        assert result["data"] == "test"
        mock_service.assert_called_once()
```

**Announce test creation:**

```
‚ú® Generating tests for Task {N}.{M}...

Test File: tests/unit/test_{component}.py (NEW)
Test Count: {Y} tests
Scenarios:
  - Core functionality ({X} tests)
  - Edge cases ({Y} tests)
  - Error handling ({Z} tests)
  - Integration points ({A} tests)

Coverage Target: 80%+
```

### Run Test Suite

**Execute tests for this task:**

```bash
# Run tests
pytest tests/unit/test_{component}.py -v

# Check coverage
pytest tests/unit/test_{component}.py \
  --cov=apex_memory.{module} \
  --cov-report=term \
  --cov-report=html
```

**Display results:**

```
Running tests for Task {N}.{M}...

Test Results:
  ‚úì test_core_functionality PASSED
  ‚úì test_edge_case_empty_input PASSED
  ‚úì test_error_handling PASSED
  ‚úì test_integration_with_external_service PASSED
  ... (all tests)

Summary:
  ‚úì {Y}/{Y} tests passing (100%)
  ‚úì Coverage: {X}% (target: 80%+)
  ‚úì No failures or errors
```

### Verify Success Criteria

**Check each criterion from task file:**

```
Verifying Task {N}.{M} success criteria:

From task file:
  1. {Criterion 1}
     ‚úì Verified: {How verified}

  2. {Criterion 2}
     ‚úì Verified: {How verified}

  3. {Criterion 3}
     ‚úì Verified: {How verified}

  ... (all criteria)

All success criteria met: ‚úÖ
```

### Quality Gate (BLOCKING)

**This gate BLOCKS progression if not met.**

#### If Tests Fail or Criteria Not Met:

```
‚ö†Ô∏è  QUALITY GATE FAILED

Cannot proceed to next task until issues resolved.

Issues Found:
  ‚ùå test_{name} FAILED
     Error: {Error message}
     Fix required: {Suggested fix}

  ‚ùå Success criterion "{Criterion}" NOT MET
     Expected: {Expected state}
     Actual: {Actual state}
     Fix required: {Suggested fix}

Actions Required:
  1. Fix failing tests
  2. Verify all success criteria
  3. Re-run validation (Step 3)

Do NOT proceed until quality gate passes.
```

**User must fix issues and validation must pass before continuing.**

#### If All Pass:

```
‚úÖ QUALITY GATE PASSED

Task {N}.{M}: {Task Name}

Validation Results:
  ‚úì All success criteria met ({X}/{X})
  ‚úì All tests passing ({Y}/{Y})
  ‚úì Code coverage: {Z}% (‚â•80% target)
  ‚úì No errors or warnings

Implementation Quality:
  ‚úì Type hints complete
  ‚úì Docstrings complete
  ‚úì Error handling implemented
  ‚úì Logging structured
  ‚úì No TODO/FIXME in production code

Ready to mark Task {N}.{M} complete and update progress.

Proceeding to Step 4...
```

**ENFORCES QUALITY** - Cannot skip failing tests or incomplete success criteria.

**Time:** 5-10 minutes per task (automated testing + verification)

---

## Step 4: Track Progress üìä

**Automatically update task-manager/ structure with completion status.**

### Update Task File

**File:** `task-manager/phase-{N}-*/task-{N}.{M}-*.md`

**Update header:**
```markdown
**Status:** ‚úÖ Complete
**Completed:** {YYYY-MM-DD}
**Actual Duration:** {X} hours (estimated: {Y} hours)
```

**Update progress tracking section:**
```markdown
## Progress Tracking

**Subtasks:** 4/4 complete (100%)

- [x] Subtask {N}.{M}.1: {Name} ‚úÖ ({Completion date})
- [x] Subtask {N}.{M}.2: {Name} ‚úÖ ({Completion date})
- [x] Subtask {N}.{M}.3: {Name} ‚úÖ ({Completion date})
- [x] Subtask {N}.{M}.4: {Name} ‚úÖ ({Completion date})

**Tests:** {Y}/{Y} passing (100%)
**Success Criteria:** {X}/{X} met (100%)
```

### Update Phase README

**File:** `task-manager/phase-{N}-*/README.md`

**Update task table:**
```markdown
| Task | Name | Status | Duration | Tests | Subtasks |
|------|------|--------|----------|-------|----------|
| {N}.1 | {Name} | ‚úÖ | 2 days | 10/10 | 4/4 |
| {N}.2 | {Name} | ‚¨ú | 1 day | 0/6 | 0/4 |
| {N}.3 | {Name} | ‚¨ú | 1 day | 0/4 | 0/4 |
```

**Update totals:**
```markdown
**Totals:**
- Tasks: 1/4 complete (25%)
- Subtasks: 4/16 complete (25%)
- Tests: 10/42 passing (24%)
```

### Update Master README

**File:** `task-manager/README.md`

**Update progress table:**
```markdown
| Phase | Name | Tasks | Subtasks | Tests | Status | Progress |
|-------|------|-------|----------|-------|--------|----------|
| {N} | {Phase Name} | 1/4 | 4/16 | 10/42 | üîÑ | 25% |
```

**Update overall progress:**
```markdown
**Overall Progress:** {X}/23 tasks ({Y}%) | {A}/89 subtasks ({B}%)
```

### Display Update Summary

```
üìä PROGRESS UPDATED

Task {N}.{M} Complete:
  ‚úì 4/4 subtasks done
  ‚úì {Y}/{Y} tests passing (100%)
  ‚úì All success criteria met
  ‚úì Completed: {YYYY-MM-DD}
  ‚úì Duration: {X} hours (estimated: {Y} hours)

Files Updated:
  ‚úì task-manager/phase-{N}-*/task-{N}.{M}-*.md
  ‚úì task-manager/phase-{N}-*/README.md
  ‚úì task-manager/README.md

Phase {N} Progress:
  ‚îú‚îÄ Tasks: {X}/{TOTAL} complete ({Y}%)
  ‚îú‚îÄ Subtasks: {A}/{TOTAL} complete ({B}%)
  ‚îî‚îÄ Tests: {C}/{TOTAL} passing ({D}%)

Overall Project Progress:
  ‚îú‚îÄ Tasks: {E}/23 complete ({F}%)
  ‚îú‚îÄ Subtasks: {G}/89 complete ({H}%)
  ‚îî‚îÄ Tests: {I}/107 passing ({J}%)

Estimated Remaining: {X} hours this phase, {Y} hours total
```

**FULLY AUTOMATED** - No manual README editing required.

**Time:** ~10 seconds (automated file updates)

---

## Step 5: Next Action üéØ

**Provide clear guidance on what to do next.**

### Determine Next Action

**Three possible states:**

1. **More tasks in current phase** ‚Üí Continue to next task
2. **Phase complete** ‚Üí Recommend context compact
3. **Project complete** ‚Üí Celebrate and prepare deployment

### State 1: More Tasks in Phase

**If tasks remaining in current phase:**

```
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
NEXT TASK
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Task {N}.{M+1}: {Task Name}

Duration: {X} hours
Tests: {Y} tests to create
Dependencies:
  ‚úì Task {N}.{M} (Complete)
  {Other dependencies if any}

Subtasks:
  ‚¨ú {N}.{M+1}.1: {Name} (4 hours)
  ‚¨ú {N}.{M+1}.2: {Name} (6 hours)
  ‚¨ú {N}.{M+1}.3: {Name} (3 hours)
  ‚¨ú {N}.{M+1}.4: {Name} (2 hours)

Phase {N} Progress: {X}/{TOTAL} tasks ({Y}%)
Estimated Remaining: {Z} hours

Ready to continue? Proceeding to Step 2 for next task...
```

**Auto-proceed to Step 2 (Execute Subtasks) for next task.**

### State 2: Phase Complete

**If all tasks in phase are done:**

```
üéâ PHASE {N} COMPLETE!

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
PHASE COMPLETION SUMMARY
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

Phase {N}: {Phase Name}
Duration: {X} weeks (estimated: {Y} weeks)
Status: ‚úÖ COMPLETE

Tasks Completed:
  ‚úÖ Task {N}.1: {Name} ({Duration}, {Tests} tests)
  ‚úÖ Task {N}.2: {Name} ({Duration}, {Tests} tests)
  ‚úÖ Task {N}.3: {Name} ({Duration}, {Tests} tests)
  ‚úÖ Task {N}.4: {Name} ({Duration}, {Tests} tests)

Deliverables:
  ‚úì {X} implementation files created/modified
  ‚úì {Y} lines of code written
  ‚úì {Z} tests created (100% passing)
  ‚úì {A} research references used
  ‚úì All success criteria met

Quality Metrics:
  ‚úì Test coverage: {X}% (‚â•80% target)
  ‚úì Code quality: All standards met
  ‚úì Documentation: 100% complete
  ‚úì No TODO/FIXME in production code

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

‚ö†Ô∏è  RECOMMENDED: Context Compact

Why compact now:
  - Phase {N} complete ({X} tasks, {Y} subtasks)
  - ~{Z} hours of implementation discussion
  - Next phase: {A} tasks, ~{B} hours of new work
  - Estimated conversation size: {C}k tokens

What gets preserved:
  ‚úì All code files created
  ‚úì All tests and validation results
  ‚úì Progress in task-manager/ (all READMEs updated)
  ‚úì Phase completion status
  ‚úì Critical architectural decisions

What gets summarized:
  - Step-by-step implementation details
  - Code generation discussions
  - Test creation iterations
  - Debugging sessions

Resume Command After Compact:
  /execute {N+1}

Options:
  1. /compact (RECOMMENDED - cleaner context for next phase)
  2. /execute {N+1} (continue without compact)
  3. Take a break (checkpoint saved, can resume anytime)

Current state saved in task-manager/ - safe to compact or pause.
```

### State 3: Project Complete

**If all phases are done:**

```
üèÜ PROJECT COMPLETE! üèÜ

‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
{PROJECT NAME} - IMPLEMENTATION COMPLETE
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

All {TOTAL} phases completed successfully!

Phase Summary:
  ‚úÖ Phase 1: {Name} ({X} tasks, {Y} tests)
  ‚úÖ Phase 2: {Name} ({A} tasks, {B} tests)
  ‚úÖ Phase 3: {Name} ({C} tasks, {D} tests)
  ... (all phases)

Final Statistics:
  ‚úì Tasks: {X}/{X} complete (100%)
  ‚úì Subtasks: {Y}/{Y} complete (100%)
  ‚úì Tests: {Z}/{Z} passing (100%)
  ‚úì Files: {A} implementation files
  ‚úì Lines: ~{B} lines of code
  ‚úì Duration: {C} weeks (estimated: {D} weeks)

Quality Achievements:
  ‚úì 100% test pass rate
  ‚úì {X}% code coverage (‚â•80% target)
  ‚úì Zero TODO/FIXME in production code
  ‚úì Complete documentation
  ‚úì All examples working

Next Steps:
  1. Review TESTING.md - Run full test suite
  2. Review DEPLOYMENT-GUIDE.md - Prepare deployment
  3. Run production readiness checklist
  4. Deploy to staging environment
  5. User acceptance testing
  6. Production deployment

Congratulations! Ready for deployment. üöÄ
```

### Smart Compact Recommendations

**Automatically suggest context compact based on:**

**Triggers:**
- ‚úÖ Each phase complete (natural break point)
- ‚úÖ Before starting tasks >16 hours (multi-day work)
- ‚úÖ When conversation >150k tokens (approaching limit)

**Compact Timing Example:**
```
üí° Smart Compact Recommendation

Compact suggested because:
  - Phase {N} complete (natural checkpoint)
  - {X} hours of detailed implementation work
  - Conversation size: ~{Y}k tokens
  - Next phase: New features, different focus area

Benefits of compacting now:
  ‚úì Cleaner context for Phase {N+1}
  ‚úì Faster Claude responses
  ‚úì Better focus on new work
  ‚úì All progress preserved in task-manager/

Checkpoint saved: phase-{N}-complete
Resume command: /execute {N+1}

Safe to compact - all state preserved in files.
```

### Create Checkpoint

**Before any potential compact or break:**

```
Checkpoint Saved: phase-{N}-complete

Current State:
  - Phase {N}: ‚úÖ Complete ({X}/{X} tasks)
  - Overall: {Y}/23 tasks ({Z}%)
  - Next: Phase {N+1} ({A} tasks, ~{B} hours)

Resume Command:
  /execute {N+1}

All progress saved in:
  ‚úì task-manager/README.md (master progress)
  ‚úì task-manager/phase-{N}-*/README.md (phase complete)
  ‚úì task-manager/phase-{N}-*/task-*.md (all tasks)

Safe to:
  - Take a break
  - Compact context
  - Switch projects

Can resume anytime with /execute {N+1}
```

**ALWAYS ORIENTED** - Developer always knows what's next, when to compact, how to resume.

**Time:** Instant (guidance display only)

---

## Quick Reference

### Command Usage

```bash
# Auto-detect next phase
/execute

# Execute specific phase
/execute 1        # Phase 1
/execute 2        # Phase 2
/execute 3        # Phase 3
```

### Prerequisites Checklist

Before running `/execute [phase]`:

```
‚òë /breakdown completed (task-manager/ exists)
‚òë Previous phases complete (check dependencies)
‚òë Development environment ready
  ‚òë Services running (databases, APIs, etc.)
  ‚òë Dependencies installed
  ‚òë Environment variables configured
‚òë Tests from previous phases still passing
```

### The 5-Step Workflow

```
1. LOAD (30 sec)
   ‚îî‚îÄ Read task files ‚Üí Show checklist

2. EXECUTE (varies)
   ‚îî‚îÄ Implement subtasks ‚Üí Quality code

3. VALIDATE (5-10 min)
   ‚îî‚îÄ Generate tests ‚Üí Verify criteria ‚Üí BLOCKING GATE

4. TRACK (10 sec)
   ‚îî‚îÄ Update READMEs ‚Üí Show progress

5. NEXT (instant)
   ‚îî‚îÄ Show next task ‚Üí Recommend compact ‚Üí Save checkpoint
```

### Quality Standards

**Code Quality (Automatically Applied):**
- ‚úÖ Type hints everywhere
- ‚úÖ Google-style docstrings
- ‚úÖ Async/await where appropriate
- ‚úÖ Structured logging (no print statements)
- ‚úÖ Proper error handling (specific exceptions)
- ‚úÖ No TODO/FIXME in production code

**Testing Standards:**
- ‚úÖ Tests generated proactively (without asking)
- ‚úÖ Unit tests (component isolation)
- ‚úÖ Integration tests (component interaction)
- ‚úÖ Edge cases (boundary conditions)
- ‚úÖ Error tests (exception handling)
- ‚úÖ Coverage ‚â•80% target
- ‚úÖ 100% test pass rate (blocking gate)

**Progress Tracking:**
- ‚úÖ Fully automated README updates
- ‚úÖ Real-time progress metrics
- ‚úÖ Estimated time remaining
- ‚úÖ Checkpoint saves for safe resumption

### Time Estimates

**Per Phase:**
- Setup: ~30 seconds (Step 1)
- Implementation: Varies (Step 2 - actual coding)
- Validation: ~5-10 minutes per task (Step 3)
- Progress tracking: ~10 seconds (Step 4)
- Next action: Instant (Step 5)

**Total Overhead:** ~10 minutes per phase (vs 30+ minutes in legacy workflow)

### Success Metrics

**What This Workflow Delivers:**
- ‚úÖ Zero redundant planning (trusts /breakdown)
- ‚úÖ Quality enforced (cannot skip failing tests)
- ‚úÖ Progress automated (no manual README updates)
- ‚úÖ Always oriented (clear next steps)
- ‚úÖ Context efficient (strategic compact reminders)
- ‚úÖ Fast execution (30 sec setup vs 15-30 min)

---

## Validation Checkpoints

**Step 1 (Load):**
- [ ] task-manager/ structure detected
- [ ] All task files loaded successfully
- [ ] Dependencies verified
- [ ] Context displayed clearly

**Step 2 (Execute):**
- [ ] Subtasks executed in order
- [ ] Quality standards applied
- [ ] Code follows patterns
- [ ] Validation commands pass

**Step 3 (Validate):**
- [ ] Tests generated proactively
- [ ] All tests passing (100%)
- [ ] Success criteria met
- [ ] Quality gate enforced

**Step 4 (Track):**
- [ ] Task file updated (status, completion date)
- [ ] Phase README updated (task table, totals)
- [ ] Master README updated (progress, overall stats)
- [ ] Progress displayed accurately

**Step 5 (Next):**
- [ ] Next action clear
- [ ] Compact recommended at right time
- [ ] Checkpoint saved
- [ ] Resume command provided

---

**This workflow implements the lean execution pattern that complements /breakdown with zero redundancy and maximum efficiency.**
